{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_AUDIO_DIR = os.path.dirname(os.getcwd()) + \"/data-manipulation/input/audio/\"\n",
    "INPUT_META_DIR = os.path.dirname(os.getcwd()) + \"/data-manipulation/input/metadata/\"\n",
    "\n",
    "if not os.path.exists(INPUT_AUDIO_DIR):\n",
    "    os.makedirs(INPUT_AUDIO_DIR)\n",
    "if not os.path.exists(INPUT_META_DIR):\n",
    "    os.makedirs(INPUT_META_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions\n",
    "Mainly pulled from model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_LABELS = tf.constant(['Rctrl', 'p', 'esc', 'g', 'slash', 'down', '7', 'equal', 'w', 'a', 'dash', 'caps', 'l', 'd', 'backspace', 'bracketclose', 'z', '1', 'end', 'Rshift', 'comma', 'c', 'tab', 'b', 'j', 'right', 'Lctrl', 'n', 't', 'f', 'm', 'o', 'apostrophe', 'y', '8', 'space', 'backslash', 's', '9', 'i', 'r', 'bracketopen', 'semicolon', 'q', '5', 'k', '3', 'x', '4', '6', '2', 'Lshift', 'left', 'backtick', 'enter', 'fullstop', 'e', '0', 'h', 'v', 'up', 'u', 'delete'], dtype=tf.string)\n",
    "\n",
    "# Encodes a list of instance labels into a binary vector\n",
    "def multi_label_binary_encode_tensor(instance_labels):\n",
    "    # Ensure instance_labels is a list (to handle both single and multi-label cases)\n",
    "    if isinstance(instance_labels, tf.Tensor) and instance_labels.shape == ():\n",
    "        instance_labels = tf.expand_dims(instance_labels, axis=0)\n",
    "\n",
    "    # Ensure instance_labels is a list\n",
    "    if isinstance(instance_labels, str):\n",
    "        instance_labels = [instance_labels]\n",
    "        \n",
    "    # print(f\"Encoding labels: {instance_labels}\")  # Debug\n",
    "\n",
    "    # Create a tensor of zeros with the same length as all_labels\n",
    "    binary_vector = tf.zeros(len(ALL_LABELS), dtype=tf.int32)\n",
    "\n",
    "    # Iterate through instance_labels and set corresponding indices to 1\n",
    "    for label in instance_labels:\n",
    "        # Find the index of the label in ALL_LABELS using TensorFlow string matching\n",
    "        matches = tf.equal(ALL_LABELS, label)\n",
    "        indices = tf.where(matches)  # Indices where matches occur\n",
    "        if tf.size(indices) > 0:  # Ensure the label exists in ALL_LABELS\n",
    "            index = indices[0][0]\n",
    "            binary_vector = tf.tensor_scatter_nd_update(\n",
    "                binary_vector, indices=[[index]], updates=[1]\n",
    "            )\n",
    "    \n",
    "    return binary_vector\n",
    "\n",
    "def multi_label_binary_decode_tensor(binary_vector):\n",
    "    binary_vector = binary_vector.numpy()\n",
    "    decoded_labels = [ALL_LABELS[i] for i, val in enumerate(binary_vector) if val == 1]\n",
    "    return decoded_labels\n",
    "\n",
    "def get_waveform(filepath):\n",
    "    audio_binary = tf.io.read_file(filepath)\n",
    "    audio = tf.squeeze(audio_binary)\n",
    "    waveform, samplerate = tf.audio.decode_wav(audio)\n",
    "\n",
    "    # Reduce to 1 channel by averaging\n",
    "    waveform = tf.reduce_mean(waveform, axis=-1)\n",
    "    \n",
    "    if (samplerate != 44100):\n",
    "        print(\"Incorrect sample rate: \" + filepath)\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "def split_into_windows(waveform, frame_length=2205, frame_step=1102): # 50ms windows with 50% overlap\n",
    "    # print(\"Waveform shape before framing:\", waveform.shape)\n",
    "    frames = tf.signal.frame(waveform, frame_length=frame_length, frame_step=frame_step)\n",
    "    # print(\"Frames shape after framing (with overlap):\", frames.shape)\n",
    "    return frames\n",
    "\n",
    "def split_into_sequences(frames, sequence_length=3):\n",
    "    num_frames = tf.shape(frames)[0]\n",
    "    sequence_step = 1\n",
    "    start_indices = tf.range(0, num_frames - sequence_length + 1, sequence_step)\n",
    "    sequences = tf.map_fn(\n",
    "        lambda start: frames[start:start + sequence_length],\n",
    "        start_indices,\n",
    "        fn_output_signature=tf.TensorSpec(shape=(sequence_length, frames.shape[1]), dtype=frames.dtype)\n",
    "    )\n",
    "    # print(\"Frames grouped into sequences:\", sequences.shape)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_data():\n",
    "    data = []\n",
    "    for file in os.listdir(INPUT_AUDIO_DIR):\n",
    "        split_fn = file.split(\"-\")\n",
    "        split_fn[1] = split_fn[1][:-4] # Remove .wav extension\n",
    "        split_fn[0] = float(split_fn[0])\n",
    "        split_fn[1] = float(split_fn[1])\n",
    "        data.append({\"start_time\": datetime.fromtimestamp(split_fn[0]),\n",
    "                      \"end_time\": datetime.fromtimestamp(split_fn[1]),\n",
    "                        \"waveform\": get_waveform(INPUT_AUDIO_DIR + file)\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "def loadMetadata():\n",
    "    metadata = []\n",
    "    for file in os.listdir(INPUT_META_DIR):\n",
    "        with open(INPUT_META_DIR + file, \"r\") as file:\n",
    "            fdata = file.readlines()\n",
    "\n",
    "        for line in fdata:\n",
    "            line = line.strip(\"\\n\").split(\",\")\n",
    "            metadata.append({\"label\": multi_label_binary_encode_tensor(line[0]), \"time\": datetime.fromtimestamp(float(line[1]))})\n",
    "    return metadata\n",
    "\n",
    "# Remove keypresses keylogged outside of recording time\n",
    "def filterMetadata(audio_data, metadata):\n",
    "    filtered_metadata = []\n",
    "    for mdata in metadata:\n",
    "        for audio_ts in audio_data:\n",
    "            start_time = audio_ts[\"start_time\"]\n",
    "            end_time = audio_ts[\"end_time\"]\n",
    "            timestamp = mdata[\"time\"]\n",
    "            if start_time <= timestamp <= end_time:\n",
    "                filtered_metadata.append(mdata)\n",
    "    return filtered_metadata\n",
    "\n",
    "def matchMetadataToAudioSlice(audio_slice, metadata):\n",
    "    matched_metadata = []\n",
    "    start_time = audio_slice[\"start_time\"]\n",
    "    end_time = audio_slice[\"end_time\"]\n",
    "\n",
    "    for mdata in metadata:\n",
    "        mdata_ts = mdata[\"time\"]\n",
    "        if start_time <= mdata_ts <= end_time:\n",
    "            matched_metadata.append(mdata)\n",
    "\n",
    "    return matched_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split waveform into windows, label frames for each window, then turn windows and frames into sequences of 3\n",
    "def preprocess_waveform_and_label(audio_sample, metadata):\n",
    "    waveform = audio_sample[\"waveform\"]\n",
    "    start_time = audio_sample[\"start_time\"]    \n",
    "\n",
    "    tf.debugging.assert_equal(\n",
    "        tf.shape(waveform)[0], \n",
    "        tf.constant(44100, dtype=tf.int32),\n",
    "        message=\"Waveform must have 44100 samples\"\n",
    "    )\n",
    "\n",
    "    sample_rate = 44100\n",
    "    frame_length = 2205\n",
    "    frame_step = 1102\n",
    "    tolerance_sec = 50 / 1000.0\n",
    "\n",
    "    # Split waveform into windows\n",
    "    frames = split_into_windows(waveform, frame_length, frame_step)\n",
    "    num_frames = tf.shape(frames)[0]\n",
    "\n",
    "    frame_time_ranges = [\n",
    "        (i * frame_step / sample_rate, (i * frame_step + frame_length) / sample_rate)\n",
    "        for i in range(num_frames)\n",
    "    ]\n",
    "\n",
    "    # Assign labels\n",
    "    frame_labels = []\n",
    "    for frame_start, frame_end in frame_time_ranges:\n",
    "        frame_labels_binary = tf.zeros(len(ALL_LABELS), dtype=tf.int32)  # Default zero tensor\n",
    "\n",
    "        for label_data in metadata:\n",
    "            label_time = (label_data[\"time\"] - start_time).total_seconds()\n",
    "            if (frame_start - tolerance_sec) <= label_time <= (frame_end + tolerance_sec):\n",
    "                label_index = tf.where(ALL_LABELS == multi_label_binary_decode_tensor(label_data[\"label\"]))\n",
    "                if tf.size(label_index) > 0:\n",
    "                    frame_labels_binary = tf.tensor_scatter_nd_update(\n",
    "                        frame_labels_binary,\n",
    "                        indices=[[label_index[0][0]]],\n",
    "                        updates=[1]\n",
    "                    )\n",
    "\n",
    "        frame_labels.append(frame_labels_binary)\n",
    "\n",
    "    frame_sequences = split_into_sequences(frames, 3)\n",
    "    label_sequences = split_into_sequences(tf.stack(frame_labels), 3)\n",
    "\n",
    "    # Debug\n",
    "    # print(\"Frames shape:\", frame_sequences.shape)\n",
    "    # print(\"Labels shape:\", label_sequences.shape)\n",
    "\n",
    "    return frame_sequences, label_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(audio_data, metadata, sequence_length=3):\n",
    "    def gen():\n",
    "        for audio_slice in audio_data:\n",
    "            matched_metadata = matchMetadataToAudioSlice(audio_slice, metadata)\n",
    "            frame_sequences, label_sequences = preprocess_waveform_and_label(audio_slice, matched_metadata)\n",
    "            for x, y in zip(frame_sequences, label_sequences):\n",
    "                yield x, y\n",
    "\n",
    "    # Create a dataset from the generator\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(sequence_length, 2205), dtype=tf.float32),  # Frames\n",
    "            tf.TensorSpec(shape=(sequence_length, 63), dtype=tf.int32)  # Labels\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(3, 2205), dtype=tf.float32, name=None), TensorSpec(shape=(3, 63), dtype=tf.int32, name=None))\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "Frames shape: (37, 3, 2205)\n",
      "Labels shape: (37, 3, 63)\n",
      "1443\n"
     ]
    }
   ],
   "source": [
    "audio_data = load_audio_data()\n",
    "metadata = filterMetadata(audio_data, loadMetadata())\n",
    "\n",
    "dataset = create_dataset(audio_data, metadata)\n",
    "\n",
    "dataset.cache()\n",
    "\n",
    "print(dataset.element_spec)\n",
    "print(dataset.reduce(0, lambda x, _: x + 1).numpy())\n",
    "\n",
    "dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m ds \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequence, label_sequence \u001b[38;5;129;01min\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m     plot_spectrogram_sequence(sequence, label\u001b[38;5;241m=\u001b[39m[multi_label_binary_decode_tensor(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m label_sequence])\n",
      "Cell \u001b[0;32mIn[78], line 40\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m ds \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequence, label_sequence \u001b[38;5;129;01min\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m     plot_spectrogram_sequence(sequence, label\u001b[38;5;241m=\u001b[39m[\u001b[43mmulti_label_binary_decode_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m label_sequence])\n",
      "Cell \u001b[0;32mIn[73], line 33\u001b[0m, in \u001b[0;36mmulti_label_binary_decode_tensor\u001b[0;34m(binary_vector)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmulti_label_binary_decode_tensor\u001b[39m(binary_vector):\n\u001b[1;32m     32\u001b[0m     binary_vector \u001b[38;5;241m=\u001b[39m binary_vector\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 33\u001b[0m     decoded_labels \u001b[38;5;241m=\u001b[39m [ALL_LABELS[i] \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(binary_vector) \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoded_labels\n",
      "Cell \u001b[0;32mIn[73], line 33\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmulti_label_binary_decode_tensor\u001b[39m(binary_vector):\n\u001b[1;32m     32\u001b[0m     binary_vector \u001b[38;5;241m=\u001b[39m binary_vector\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 33\u001b[0m     decoded_labels \u001b[38;5;241m=\u001b[39m [ALL_LABELS[i] \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(binary_vector) \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoded_labels\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "def plot_spectrogram_sequence(sequence, sample_rate=44100, frame_length=2205, frame_step=1102, label=None):\n",
    "    sequence_length = sequence.shape[0]\n",
    "    \n",
    "    plt.figure(figsize=(15, 3 * sequence_length))  # Adjust figure size based on sequence length\n",
    "\n",
    "    for i, frame in enumerate(sequence):\n",
    "        # Compute STFT (Short-Time Fourier Transform)\n",
    "        stft = tf.signal.stft(\n",
    "            frame,\n",
    "            frame_length=frame_length // 4,\n",
    "            frame_step=frame_step // 4,\n",
    "            fft_length=frame_length // 2,\n",
    "        )\n",
    "        \n",
    "        # Compute magnitude spectrogram\n",
    "        spectrogram = tf.abs(stft)\n",
    "        spectrogram = tf.transpose(spectrogram)  # Transpose for better visualization\n",
    "\n",
    "        # Plot the spectrogram\n",
    "        plt.subplot(sequence_length, 1, i + 1)\n",
    "        plt.imshow(\n",
    "            10 * tf.math.log1p(spectrogram).numpy(),  # Log-scaled spectrogram\n",
    "            aspect='auto',\n",
    "            origin='lower',\n",
    "            extent=[0, spectrogram.shape[1], 0, sample_rate / 2]\n",
    "        )\n",
    "        plt.colorbar(label=\"Magnitude (dB)\")\n",
    "        title = f\"Frame {i + 1}\"\n",
    "        if label is not None:\n",
    "            title += f\" | Label: {label[i]}\"\n",
    "        plt.title(title)\n",
    "        plt.ylabel(\"Frequency (Hz)\")\n",
    "        plt.xlabel(\"Time (frames)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "ds = dataset.shuffle(10000)\n",
    "for sequence, label_sequence in ds.take(5):\n",
    "    plot_spectrogram_sequence(sequence, label=[multi_label_binary_decode_tensor(x) for x in label_sequence])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs528-autokeras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
